# -*- coding: utf-8 -*-
"""cancerproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NH9UK87lwGNWYMEEiANUkS-OpPuzB0t_
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn import model_selection
from sklearn.metrics import accuracy_score
from pandas import Series, DataFrame
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics
import scipy.stats as stats

raw_df=pd.read_csv("/content/cancer.csv")
df=raw_df.copy(deep=True)
df.head()

df.info()

def drop(text):
  df.drop(text,axis=1,inplace=True)
  return df
  
drop("Unnamed: 32")
drop("id")

df1 = df.copy(deep = True)
df1.info()

df1['diagnosis'].unique()

df1['diagnosis']=df1['diagnosis'].map({"M":1,"B":2})

#g = sns.PairGrid(df.iloc[:, 1:12],hue='diagnosis',palette="husl")
#g=g.map_diag(plt.hist,edgecolor="w")
#g=g.map_offdiag(plt.scatter, edgecolor="w" )
#plt.show()

sns.distplot(df1.radius_mean)

sns.distplot(df1.radius_se)

sns.distplot(df1.radius_worst)

sns.distplot(df1.texture_mean)

sns.distplot(df1.perimeter_se)

sns.distplot(df1.compactness_mean)

#multivarient analysis
sns.jointplot(df1.loc[:, 'radius_mean'], df1.loc[:, 'radius_worst'], kind= 'reg')

sns.jointplot(df1.loc[:, 'radius_mean'], df1.loc[:, 'perimeter_worst'],color='green',kind='reg')

sns.countplot(df1.diagnosis)
plt.show()

df_m=df1.iloc[:,1:11]
df_m.info()

df_se=df1.iloc[:,12:21]
df_se.info()

df_w=df1.iloc[:,21:32]
df_w.info()

df1.describe().T

#  mean
plt.figure(figsize = (15,7))
sns.heatmap(df_m.corr(), annot = True, linewidth = 0.5)
plt.show()

#se
plt.figure(figsize = (15,7))
sns.heatmap(df_se.corr(), annot = True, linewidth = 0.5)
plt.show()

#worst
plt.figure(figsize = (15,7))
sns.heatmap(df_w.corr(), annot = True, linewidth = 0.5)
plt.show()

drop_datas=["radius_mean","texture_mean","perimeter_mean","area_mean","area_se","radius_worst",
            "texture_worst","perimeter_worst","area_worst"]

df2_clean=df1.copy(deep=True)

df2_clean.info()

#preprocessing 
from sklearn.preprocessing import MinMaxScaler

y=df2_clean['diagnosis']
x=df2_clean.iloc[:,1:]

scaler = MinMaxScaler()
scaler.fit(x)

x = scaler.fit_transform(x)
x

y

#spliting, traning  and testing the datas
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state = 50)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

from sklearn.svm import SVC

#SVC model
svc_model=SVC(kernel='rbf',gamma=0.0001, C=100)
svc_model.fit(x_train,y_train)

#predict
y_prediction=svc_model.predict(x_test)
y_prediction

#Accuracy for the testing datas

print("The Accuracy of SVC for tesing data is:",metrics.accuracy_score(y_prediction,y_test))

#Accuracy of the training datas
y_p_train=svc_model.predict(x_train)

print("The Accuracy of SVC for traning data is:",accuracy_score(y_train,y_p_train))

#confusion matrix

from sklearn.metrics import confusion_matrix
from sklearn.metrics._plot.confusion_matrix import confusion_matrix

cm = confusion_matrix(y_test,y_prediction)
cm
sns.heatmap(cm,annot=True)

#precision
from sklearn.metrics import classification_report

print(classification_report(y_test,y_prediction))

#sensitivity for true positive rate
sensitivity_p= cm[0,0]/(cm[0,0]+cm[1,0])
sensitivity_p

#sensitivity for false negative rate
sensitivity_n= cm[1,0]/(cm[0,0]+cm[1,0])
sensitivity_n

#Specificity for true negative rate
specificity_n= cm[1,1]/(cm[0,1]+cm[1,1])
specificity_n

#Specificity for false positive rate
specificity_p=1-specificity_n
specificity_p

#ROC and AUC 
from sklearn.metrics import roc_curve

#ROC
fls_pr, true_pr, thresh = roc_curve(y_test, y_prediction[:], pos_label=1)
fls_pr1, true_pr1, thresh1 = roc_curve(y_train, y_p_train[:], pos_label=1)

#Auc
from sklearn.metrics import roc_auc_score
 
# auc scores for test
auc_score = roc_auc_score(y_test, y_prediction[:])

print("AUC score for tesing data is:",auc_score)
# auc scores for train
auc_score_train = roc_auc_score(y_train, y_p_train[:])

print("AUC score for training data is:",auc_score_train)

plt.plot(fls_pr, true_pr,color='blue', label='Test ROC&AUC curve area = %0.2f'%auc_score)
plt.plot(fls_pr1, true_pr1,linestyle='--',color='green', label='Train ROC&AUC curve area = %0.2f'%auc_score_train)
plt.title("ROC&AUC curve")
plt.plot([0,1],[0,1], 'r--')
plt.xlabel('False Positive Rate', size=14)
plt.ylabel('True Positive Rate', size=14)
plt.legend(loc='upper left')
plt.show()

#hyperparameter turning
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [1, 1, 1, 10, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 
  
grid = GridSearchCV(svc_model, param_grid, refit = True, verbose = 3)

grid.fit(x_train, y_train)

print(grid.best_params_)
print(grid.best_estimator_)

grid_pred = grid.predict(x_test)
  
# print classification report
print(classification_report(y_test, grid_pred))